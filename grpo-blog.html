<!DOCTYPE html>
<html lang="en">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="content-type" content="text/html; charset=UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="description" content="Jasper's Site" />
  <title>Jasper Gilley</title>

  <meta property="og:url" content="https://jagilley.github.io" />
  <meta property="og:type" content="website" />
  <meta property="og:title" content="Jasper Gilley" />
  <meta property="og:description" content="" />
  <meta property="og:site_name" content="jagilley.github.io" />
  <meta property="og:image" content="/img" />
  <link rel="canonical" href="https://jagilley.github.io/grpo-blog.html" />
  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico"/>

  <link rel="stylesheet" href="https://latex.now.sh/style.css">
</head>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<body class="latex-dark">
<h1>
    Why You're Probably Doing GRPO Wrong
</h1>
<p class="author">Jasper Gilley<br><a href="https://twitter.com/0xjasper">Twitter</a><br><a href="https://github.com/jagilley">Github</a><br></p>

<p>
    If you don't have your head in the sand, you're probably aware that possibly the most consequential takeaway from the R1 paper was their introduction of GRPO (Group Relative Policy Optimization) — RL for LLMs without the baggage of a separate value function. But if you're using GRPO as "better PPO for reasoning tasks", you're probably doing it wrong.
</p>

<p>
    The original GRPO paper focuses on mathematical reasoning. Get a dataset, train a reward model on correct/incorrect answers, unleash GRPO, profit. This works, <em>up to a point</em>. But it still relies on the ability of your dataset to encode a reward signal that is somewhat more advanced but not too much more advanced than what your model can currently handle. This is not true open-endedness.
</p>

<h3>
    The Fixed Dataset Dead End
</h3>

<p>
    The standard RL approach on fixed datasets — including that pursued by DeepSeek in training R1 — hits the same wall that SFT has for years.  You're training a model to maximize its score on a <em>finite</em> set of examples. As the model's performance saturates the input benchmark, the ability of that benchmark to provide your model with signal erodes.
</p>

<h3>
    Open-ended Rewards
</h3>

<p>
    Here's where things get interesting. Recent advances in LLMs, particularly models like Anthropic's Claude 3.5 Sonnet, have produced a surprising asymmetry: <em>they're much better at judging quality than generating it.</em>
</p>

<p>
    Think about it. To <em>write</em> a truly great novel, you need a vast amount of world knowledge, a deep understanding of human nature, mastery of language, and the ability to construct a compelling narrative arc.  That's <em>hard</em>. But to <em>recognize</em> a great novel?  That's still hard, but significantly <em>less</em> hard. You don't need to be Shakespeare to know that Shakespeare is good.
</p>

<p>
    This asymmetry is the key.  We can leverage these powerful LLMs as <em>judges</em>, providing <em>explicit</em> feedback on the qualities we care about – creativity, coherence, insightfulness, engagement. This is a <em>fundamentally different</em> kind of training signal than SFT provides.
</p>

<h3>
    GRPO: The Missing Piece
</h3>

<p>
    This is where GRPO comes in.  GRPO's elegance lies in its simplicity.  By ditching the value function and using a group-relative baseline, it provides a clean, efficient way to do RL.  But the real magic happens when you combine it with a judge LLM.
</p>

<p>
    Instead of optimizing for "correctness" on a fixed dataset, you're optimizing for <em>qualitative judgments</em> from a sophisticated LLM.  This breaks the SFT trap.  You're no longer limited by the examples in your training data.  You're teaching the model to <em>think</em>, to <em>plan</em>, to <em>create</em> in a way that satisfies a nuanced, high-level judge.
</p>

<p>
    Here's the breakdown:
</p>

<ol>
    <li><strong>SFT:</strong> Implicit feedback through examples. Mimicry. Limited generalization. Overfitting.</li>
    <li><strong>RL on Fixed Datasets:</strong> Explicit feedback (correct/incorrect), but still limited by the dataset. Overfitting.</li>
    <li><strong>GRPO + Judge LLM:</strong> Explicit, <em>qualitative</em> feedback.  Optimization for desired qualities.  Potential for <em>unbounded</em> improvement and true generalization.</li>
</ol>

<h3>
    Prompt Engineering: The New Architecture
</h3>

<p>
    The crucial skill here becomes <em>prompt engineering</em>. You're not just writing code anymore; you're crafting instructions for the judge LLM. You're defining, in precise terms, what constitutes "good" writing, "insightful" analysis, or "engaging" content. You're building a rubric, a constitution, for your AI.
</p>

<p>
    This is <em>hard</em>. It requires a deep understanding of the qualities you're trying to cultivate. It requires iterative refinement.  It requires a willingness to experiment. But the payoff is potentially enormous.
</p>

<h3>
    The Takeaway
</h3>

<p>
    Don't use GRPO to do slightly better SFT. Use it to build something <em>fundamentally new</em>. Use it to train models that can generate truly novel, insightful, and engaging content, guided by the judgment of the best LLMs we have. Stop optimizing for clicks. Start optimizing for <em>meaning</em>. This is the real revolution, and it's just getting started. The future isn't about bigger datasets; it's about better judgment.
</p>

</body>
</html>
